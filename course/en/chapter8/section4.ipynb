{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHwZgP4X7XLz"
      },
      "source": [
        "# Debugging the training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9R3Q0597XL1"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUCgw9QC7XL1"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bO2zBJaX7XL2",
        "outputId": "b505f9b0-ffb1-4872-d4b0-2342077060d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ValueError: You have to specify either input_ids or inputs_embeds'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=raw_datasets[\"train\"],\n",
        "    eval_dataset=raw_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb73Qy467XL2",
        "outputId": "37dac8bb-6d40-4ac1-b285-c9dcdf10088c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
              " 'idx': 0,\n",
              " 'label': 1,\n",
              " 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEutmkGy7XL2",
        "outputId": "7d4c8d07-230f-412c-b969-0910b6abf6c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'ValueError: expected sequence of length 43 at dim 1 (got 37)'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HlEbG3W7XL3",
        "outputId": "b6f2e059-73a1-4494-cff5-5fcbb43d6ca5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqC2oVVC7XL3",
        "outputId": "78726e70-5a0a-4f29-c7c7-68a4d87dd49c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApGXcw4K7XL3",
        "outputId": "250cb441-8041-4bb0-8b53-d7e81726eb89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(trainer.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KMBZZ3V7XL3",
        "outputId": "ba0fcfcb-4b17-4d31-aadd-bf19f4a7b0b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0][\"attention_mask\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqJkNEOI7XL3",
        "outputId": "7d1c7c31-0cd1-4f74-ae90-1e8139f195ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(trainer.train_dataset[0][\"attention_mask\"]) == len(\n",
        "    trainer.train_dataset[0][\"input_ids\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHvpE8Fh7XL3",
        "outputId": "1f5e0ad8-c94f-44d1-d1c5-44f2031ef928"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset[0][\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFgkoHcl7XL3",
        "outputId": "6c96f7bc-33ae-4996-f3ed-ccf409a03b34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['entailment', 'neutral', 'contradiction']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train_dataset.features[\"label\"].names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGyMkY2l7XL4",
        "outputId": "6927d244-bfa0-476e-c0b2-f946f1670dba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)\n",
              "    105                 batch[k] = torch.stack([f[k] for f in features])\n",
              "    106             else:\n",
              "--> 107                 batch[k] = torch.tensor([f[k] for f in features])\n",
              "    108 \n",
              "    109     return batch\n",
              "\n",
              "ValueError: expected sequence of length 45 at dim 1 (got 76)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpLvVgpa7XL4",
        "outputId": "c8aa9066-ed45-4adc-84c4-e657b1f8541f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "data_collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JMCejnY7XL4",
        "outputId": "36c5a4ef-a679-495d-9e4a-9f1fcc3e2a77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Liwbr2-Q7XL4"
      },
      "outputs": [],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "batch = data_collator([trainer.train_dataset[i] for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugx6wLwL7XL4"
      },
      "outputs": [],
      "source": [
        "data_collator = trainer.get_train_dataloader().collate_fn\n",
        "actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)\n",
        "batch = data_collator([actual_train_set[i] for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7f8IDZx7XL4"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt-8JV5Y7XL4",
        "outputId": "aa9a755b-898c-4c9a-ff0a-b4c79f040564"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n",
              "   2386         )\n",
              "   2387     if dim == 2:\n",
              "-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
              "   2389     elif dim == 4:\n",
              "   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n",
              "\n",
              "IndexError: Target 2 is out of bounds."
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = trainer.model.cpu()(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ennxf-Sr7XL4",
        "outputId": "a4dd3f8c-77f6-4aad-bb7c-19ac5360a932"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model.config.num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DbipWjX7XL4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIKbCxls7XL4"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break\n",
        "\n",
        "outputs = trainer.model.cpu()(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHaDxviL7XL4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "outputs = trainer.model.to(device)(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlcAmuqt7XL4"
      },
      "outputs": [],
      "source": [
        "loss = outputs.loss\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk96vC-D7XL4"
      },
      "outputs": [],
      "source": [
        "trainer.create_optimizer()\n",
        "trainer.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ7-QxIz7XL4",
        "outputId": "2f29364d-9d8e-4661-a9b4-8c8b28359b0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This will take a long time and error out, so you shouldn't run this cell\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdVg5t3X7XL4",
        "outputId": "41e3e0ea-c3c2-4760-d306-437d4bb157ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG-r1CNi7XL4"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_eval_dataloader():\n",
        "    break\n",
        "\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = trainer.model(**batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEKKuhfo7XL4",
        "outputId": "902ce570-e34c-4cfe-f574-f6a283ac13a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TypeError: only size-1 arrays can be converted to Python scalars"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions = outputs.logits.cpu().numpy()\n",
        "labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "compute_metrics((predictions, labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03DfyPMW7XL4",
        "outputId": "d58ce41c-7c4c-45ff-b6e1-4b0014aca5f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8, 3), (8,))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions.shape, labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNirFKx07XL5",
        "outputId": "3029ee30-5c44-4620-d2d9-45926b5bf35a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.625}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "compute_metrics((predictions, labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIuNj5te7XL5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEiPSlw37XL5"
      },
      "outputs": [],
      "source": [
        "for batch in trainer.get_train_dataloader():\n",
        "    break\n",
        "\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "trainer.create_optimizer()\n",
        "\n",
        "for _ in range(20):\n",
        "    outputs = trainer.model(**batch)\n",
        "    loss = outputs.loss\n",
        "    loss.backward()\n",
        "    trainer.optimizer.step()\n",
        "    trainer.optimizer.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NMobUF87XL5",
        "outputId": "88568351-d2d9-4a06-ccd9-831548db2900"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 1.0}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = trainer.model(**batch)\n",
        "preds = outputs.logits\n",
        "labels = batch[\"labels\"]\n",
        "\n",
        "compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Debugging the training pipeline",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}